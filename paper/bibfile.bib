
@article{chang_reading_nodate,
	title = {Reading {Tea} {Leaves}: {How} {Humans} {Interpret} {Topic} {Models}},
	abstract = {Probabilistic topic models are a popular tool for the unsupervised analysis of text, providing both a predictive model of future text and a latent topic representation of the corpus. Practitioners typically assume that the latent space is semantically meaningful. It is used to check models, summarize the corpus, and guide exploration of its contents. However, whether the latent space is interpretable is in need of quantitative evaluation. In this paper, we present new quantitative methods for measuring semantic meaning in inferred topics. We back these measures with large-scale user studies, showing that they capture aspects of the model that are undetected by previous measures of model quality based on held-out likelihood. Surprisingly, topic models which perform better on held-out likelihood may infer less semantically meaningful topics.},
	language = {en},
	author = {Chang, Jonathan and Boyd-Graber, Jordan and Gerrish, Sean and Wang, Chong and Blei, David M},
	pages = {10},
	file = {Chang et al. - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:/Users/joost/Zotero/storage/PGL3A53R/Chang et al. - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:application/pdf},
}

@inproceedings{hofmann1999probabilistic,
  title={Probabilistic latent semantic indexing},
  author={Hofmann, Thomas},
  booktitle={Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={50--57},
  year={1999}
}

@article{dumais1994latent,
  title={Latent semantic indexing (LSI) and TREC-2},
  author={Dumais, Susan T and others},
  journal={Nist Special Publication Sp},
  pages={105--105},
  year={1994},
  publisher={NATIONAL INSTIUTE OF STANDARDS \& TECHNOLOGY}
}

@article{maier_how_2020,
	title = {How {Document} {Sampling} and {Vocabulary} {Pruning} {Affect} the {Results} of {Topic} {Models}},
	volume = {2},
	copyright = {Copyright (c) 2020 Daniel Maier, Andreas Niekler, Gregor Wiedemann, Daniela Stoltenberg},
	abstract = {Topic modeling enables researchers to explore large document corpora. However, accurate model specification requires the calculation of multiple models, which can become infeasibly costly in terms of time and computing resources. In order to circumvent this problem, we test and propose a strategy introducing two easy-to-implement modifications to the modeling process: Instead of modeling the full corpus and the whole vocabulary, we (1) use random document samples and (2) an extensively pruned vocabulary. Using three empirical corpora with different origins and characteristics (news articles, websites, and Tweets), we investigate how different sample sizes and pruning strategies affect the resulting topic models as compared to fully modeled corpora. Our test provides evidence that sampling and pruning are cheap and viable strategies to accelerate model specification. Sample-based topic models closely resemble corpus-based models, if the sample size is large enough (usually \&gt;10\%). Also, extensive pruning does not compromise the quality of the resulting topics. Altogether, pruning and sample-based modeling leads to increased performance without impairing model quality.},
	language = {en},
	number = {2},
	journal = {Computational Communication Research},
	author = {Maier, Daniel and Niekler, Andreas and Wiedemann, Gregor and Stoltenberg, Daniela},
	month = nov,
	year = {2020},
	pages = {139--152},
	file = {Full Text PDF:/Users/joost/Zotero/storage/NMEILZKY/Maier et al. - 2020 - How Document Sampling and Vocabulary Pruning Affec.pdf:application/pdf;Snapshot:/Users/joost/Zotero/storage/6JWSYXJG/32.html:text/html},
}

@article{maier_applying_2018,
	title = {Applying {LDA} {Topic} {Modeling} in {Communication} {Research}: {Toward} a {Valid} and {Reliable} {Methodology}},
	volume = {12},
	issn = {1931-2458},
	shorttitle = {Applying {LDA} {Topic} {Modeling} in {Communication} {Research}},
	doi = {10.1080/19312458.2018.1430754},
	abstract = {Latent Dirichlet allocation (LDA) topic models are increasingly being used in communication research. Yet, questions regarding reliability and validity of the approach have received little attention thus far. In applying LDA to textual data, researchers need to tackle at least four major challenges that affect these criteria: (a) appropriate pre-processing of the text collection; (b) adequate selection of model parameters, including the number of topics to be generated; (c) evaluation of the model’s reliability; and (d) the process of validly interpreting the resulting topics. We review the research literature dealing with these questions and propose a methodology that approaches these challenges. Our overall goal is to make LDA topic modeling more accessible to communication researchers and to ensure compliance with disciplinary standards. Consequently, we develop a brief hands-on user guide for applying LDA topic modeling. We demonstrate the value of our approach with empirical data from an ongoing research project.},
	number = {2-3},
	journal = {Communication Methods and Measures},
	author = {Maier, Daniel and Waldherr, A. and Miltner, P. and Wiedemann, G. and Niekler, A. and Keinert, A. and Pfetsch, B. and Heyer, G. and Reber, U. and Häussler, T. and Schmid-Petri, H. and Adam, S.},
	month = apr,
	year = {2018},
	pages = {93--118},
	file = {Snapshot:/Users/joost/Zotero/storage/CTTFF96W/19312458.2018.html:text/html;Full Text:/Users/joost/Zotero/storage/NXNFHCNU/Maier et al. - 2018 - Applying LDA Topic Modeling in Communication Resea.pdf:application/pdf},
}

@article{syed_narrow_2018,
	title = {Narrow lenses for capturing the complexity of fisheries: {A} topic analysis of fisheries science from 1990 to 2016},
	volume = {19},
	copyright = {© 2018 The Authors. Fish and Fisheries Published by John Wiley \& Sons Ltd},
	issn = {1467-2979},
	shorttitle = {Narrow lenses for capturing the complexity of fisheries},
	doi = {https://doi.org/10.1111/faf.12280},
	abstract = {Despite increased fisheries science output and publication outlets, the global crisis in fisheries management is as present as ever. Since a narrow research focus may be a contributing factor to this failure, this study uncovers topics in fisheries research and their trends over time. This interdisciplinary research evaluates whether science is diversifying fisheries research topics in an attempt to capture the complexity of the fisheries system, or whether it is multiplying research on similar topics, attempting to achieve an in-depth, but possibly marginal, understanding of a few selected components of this system. By utilizing latent Dirichlet allocation as a generative probabilistic topic model, we analyse a unique dataset consisting of 46,582 full-text articles published in the last 26 years in 21 specialized scientific fisheries journals. Among the 25 topics uncovered by the model, only one (Fisheries management) refers to the human dimension of fisheries understood as socio-ecological complex adaptive systems. The most prevalent topics in our dataset directly relating to fisheries refer to Fisheries management, Stock assessment, and Fishing gear, with Fisheries management attracting the most interest. We propose directions for future research focus that most likely could contribute to providing useful advice for successful management of fisheries.},
	language = {en},
	number = {4},
	journal = {Fish and Fisheries},
	author = {Syed, Shaheen and Borit, Melania and Spruit, Marco},
	year = {2018},
	keywords = {fisheries publications, fisheries research focus, fisheries research gaps, fisheries research topics, latent Dirichlet allocation, topic modelling},
	pages = {643--661},
	file = {Snapshot:/Users/joost/Zotero/storage/WWK9YW56/faf.html:text/html;Full Text PDF:/Users/joost/Zotero/storage/IJE28X7B/Syed et al. - 2018 - Narrow lenses for capturing the complexity of fish.pdf:application/pdf},
}

@article{jacobi_quantitative_2016,
	title = {Quantitative analysis of large amounts of journalistic texts using topic modelling},
	volume = {4},
	issn = {2167-0811},
	doi = {10.1080/21670811.2015.1093271},
	abstract = {The huge collections of news content which have become available through digital technologies both enable and warrant scientific inquiry, challenging journalism scholars to analyse unprecedented amounts of texts. We propose Latent Dirichlet Allocation (LDA) topic modelling as a tool to face this challenge. LDA is a cutting edge technique for content analysis, designed to automatically organize large archives of documents based on latent topics, measured as patterns of word (co-)occurrence. We explain how this technique works, how different choices by the researcher affect the results and how the results can be meaningfully interpreted. To demonstrate its usefulness for journalism research, we conducted a case study of the New York Times coverage of nuclear technology from 1945 to the present, partially replicating a study by Gamson and Modigliani. This shows that LDA is a useful tool for analysing trends and patterns in news content in large digital news archives relatively quickly.},
	number = {1},
	journal = {Digital Journalism},
	author = {Jacobi, Carina and Atteveldt, Wouter van and Welbers, Kasper},
	month = jan,
	year = {2016},
	keywords = {automatic content analysis, Corrigendum, journalism, nuclear energy, topic models},
	pages = {89--106},
	file = {Snapshot:/Users/joost/Zotero/storage/ZWX53AWM/21670811.2015.html:text/html;Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf:/Users/joost/Zotero/storage/35PIJJKC/Jacobi et al. - 2016 - Quantitative analysis of large amounts of journali.pdf:application/pdf},
}

@article{zhang_detecting_2017,
	title = {Detecting and predicting the topic change of {Knowledge}-based {Systems}: {A} topic-based bibliometric analysis from 1991 to 2016},
	volume = {133},
	issn = {0950-7051},
	shorttitle = {Detecting and predicting the topic change of {Knowledge}-based {Systems}},
	doi = {10.1016/j.knosys.2017.07.011},
	abstract = {The journal Knowledge-based Systems (KnoSys) has been published for over 25 years, during which time its main foci have been extended to a broad range of studies in computer science and artificial intelligence. Answering the questions: “What is the KnoSys community interested in?” and “How does such interest change over time?” are important to both the editorial board and audience of KnoSys. This paper conducts a topic-based bibliometric study to detect and predict the topic changes of KnoSys from 1991 to 2016. A Latent Dirichlet Allocation model is used to profile the hotspots of KnoSys and predict possible future trends from a probabilistic perspective. A model of scientific evolutionary pathways applies a learning-based process to detect the topic changes of KnoSys in sequential time slices. Six main research areas of KnoSys are identified, i.e., expert systems, machine learning, data mining, decision making, optimization, and fuzzy, and the results also indicate that the interest of KnoSys communities in the area of computational intelligence is raised, and the ability to construct practical systems through knowledge use and accurate prediction models is highly emphasized. Such empirical insights can be used as a guide for KnoSys submissions.},
	language = {en},
	journal = {Knowledge-Based Systems},
	author = {Zhang, Yi and Chen, Hongshu and Lu, Jie and Zhang, Guangquan},
	month = oct,
	year = {2017},
	keywords = {Bibliometrics, Knowledge-based Systems, Text mining, Topic analysis, Topic detection and tracking},
	pages = {255--268},
	file = {ScienceDirect Snapshot:/Users/joost/Zotero/storage/32P93EHR/S0950705117303271.html:text/html;Submitted Version:/Users/joost/Zotero/storage/UHHH92T2/Zhang et al. - 2017 - Detecting and predicting the topic change of Knowl.pdf:application/pdf},
}

@article{dimaggio_exploiting_2013,
	series = {Topic {Models} and the {Cultural} {Sciences}},
	title = {Exploiting affinities between topic modeling and the sociological perspective on culture: {Application} to newspaper coverage of {U}.{S}. government arts funding},
	volume = {41},
	issn = {0304-422X},
	shorttitle = {Exploiting affinities between topic modeling and the sociological perspective on culture},
	doi = {10.1016/j.poetic.2013.08.004},
	abstract = {Topic modeling provides a valuable method for identifying the linguistic contexts that surround social institutions or policy domains. This article uses Latent Dirichlet Allocation (LDA) to analyze how one such policy domain, government assistance to artists and arts organizations, was framed in almost 8000 articles. These comprised all articles that referred to government support for the arts in the U.S. published in five U.S. newspapers between 1986 and 1997—a period during which such assistance, once noncontroversial, became a focus of contention. We illustrate the strengths of topic modeling as a means of analyzing large text corpora, discuss the proper choice of models and interpretation of model results, describe means of validating topic-model solutions, and demonstrate the use of topic models in combination with other statistical tools to estimate differences between newspapers in the prevalence of different frames. Throughout, we emphasize affinities between the topic-modeling approach and such central concepts in the study of culture as framing, polysemy, heteroglossia, and the relationality of meaning.},
	language = {en},
	number = {6},
	journal = {Poetics},
	author = {DiMaggio, Paul and Nag, Manish and Blei, David},
	month = dec,
	year = {2013},
	keywords = {Content analysis, Heteroglossia, Meaning, National Endowment for the Arts, Polysemy, Topic models},
	pages = {570--606},
	file = {ScienceDirect Snapshot:/Users/joost/Zotero/storage/LXRUK7S2/S0304422X13000661.html:text/html},
}

@inproceedings{rehurek_software_2010,
	title = {Software {Framework} for {Topic} {Modelling} with {Large} {Corpora}},
	abstract = {Large corpora are ubiquitous in today’s world and memory quickly becomes the limiting factor in practical applications of the Vector Space Model (VSM). In this paper, we identify a gap in existing implementations of many of the popular algorithms, which is their scalability and ease of use. We describe a Natural Language Processing software framework which is based on the idea of document streaming, i.e. processing corpora document after document, in a memory independent fashion. Within this framework, we implement several popular algorithms for topical inference, including Latent Semantic Analysis and Latent Dirichlet Allocation, in a way that makes them completely independent of the training corpus size. Particular emphasis is placed on straightforward and intuitive framework design, so that modifications and extensions of the methods and/or their application by interested practitioners are effortless. We demonstrate the usefulness of our approach on a real-world scenario of computing document similarities within an existing digital library DML-CZ. 1.},
	booktitle = {In {Proceedings} of the {Lrec} 2010 {Workshop} on {New} {Challenges} for {Nlp} {Frameworks}},
	author = {Rehurek, Radim and Sojka, Petr},
	year = {2010},
	pages = {45--50},
	file = {Citeseer - Snapshot:/Users/joost/Zotero/storage/5BQD3UCZ/summary.html:text/html;Citeseer - Full Text PDF:/Users/joost/Zotero/storage/7L2GRZDN/Rehurek and Sojka - 2010 - Software Framework for Topic Modelling with Large .pdf:application/pdf},
}

@article{chen_survey_2016,
	title = {A survey on the use of topic models when mining software repositories},
	volume = {21},
	issn = {1573-7616},
	doi = {10.1007/s10664-015-9402-8},
	abstract = {Researchers in software engineering have attempted to improve software development by mining and analyzing software repositories. Since the majority of the software engineering data is unstructured, researchers have applied Information Retrieval (IR) techniques to help software development. The recent advances of IR, especially statistical topic models, have helped make sense of unstructured data in software repositories even more. However, even though there are hundreds of studies on applying topic models to software repositories, there is no study that shows how the models are used in the software engineering research community, and which software engineering tasks are being supported through topic models. Moreover, since the performance of these topic models is directly related to the model parameters and usage, knowing how researchers use the topic models may also help future studies make optimal use of such models. Thus, we surveyed 167 articles from the software engineering literature that make use of topic models. We find that i) most studies centre around a limited number of software engineering tasks; ii) most studies use only basic topic models; iii) and researchers usually treat topic models as black boxes without fully exploring their underlying assumptions and parameter values. Our paper provides a starting point for new researchers who are interested in using topic models, and may help new researchers and practitioners determine how to best apply topic models to a particular software engineering task.},
	language = {en},
	number = {5},
	journal = {Empirical Software Engineering},
	author = {Chen, Tse-Hsun and Thomas, Stephen W. and Hassan, Ahmed E.},
	month = oct,
	year = {2016},
	pages = {1843--1919},
}

@misc{syed_topic_2019,
	type = {Dissertation},
	title = {Topic {Discovery} from {Textual} {Data}: {Machine} {Learning} and {Natural} {Language} {Processing} for {Knowledge} {Discovery} in the {Fisheries} {Domain}},
	copyright = {Open Access (free)},
	shorttitle = {Topic {Discovery} from {Textual} {Data}},
	url = {https://dspace.library.uu.nl/handle/1874/374917},
	abstract = {It is estimated that the world’s data will increase to roughly 160 billion terabytes by 2025, with most of that data occurring in an unstructured form. Today, we have already reached the point where more data is being produced than can be physically stored. To ingest all this data and to construct valuable knowledge from it, new computational tools and algorithms are needed, especially since manual probing of the data is slow, expensive, and subjective. For unstructured data, such as text in documents, an ongoing field of research is probabilistic topic models. Topic models are techniques to automatically uncover the hidden or latent topics present within a collection of documents. Topic models can infer the topical content of thousands or millions of documents without prior labeling or annotation. This unsupervised nature makes probabilistic topic models a useful tool for applied data scientists to interpret and examine large volumes of documents for extracting new and valuable knowledge. This dissertation scientifically investigates how to optimally and efficiently apply and interpret topic models to large collections of documents. Specifically, it shows how different types of textual data, pre-processing steps, and hyper-parameter settings can affect the quality of the derived latent topics. The results presented in this dissertation provide a starting point for researchers who want to apply topic models with scientific rigorousness to scientific publications.},
	language = {en},
	urldate = {2021-02-05},
	author = {Syed, S.},
	month = mar,
	year = {2019},
	note = {Accepted: 2019-02-01T17:21:47Z
ISBN: 9789039370865
Publisher: Utrecht University},
	file = {Full Text PDF:/Users/joost/Zotero/storage/IHNKNHTR/Syed - 2019 - Topic Discovery from Textual Data Machine Learnin.pdf:application/pdf;Snapshot:/Users/joost/Zotero/storage/TZWEGCSF/374917.html:text/html},
}

@inproceedings{van_dijk_pillars_nodate,
	title = {Pillars of {Privacy}: {Identifying} {Core} {Theory} in a {Network} {Analysis} of {Privacy} {Literature}},
	abstract = {Privacy research is divided in distinct communities. It is rarely considered as a singular field, making it difficult to get an overview of the disciplinary identity of privacy. We use a bibliometric network analysis to investigate the privacy research field and identify its core theories in a quantitative (positivist) manner, followed by qualitative analysis. The network consists of 83,159 publications with 462,633 references between them, spread over 90 identifiable disciplines. Centrality measures provide the 112 most influential publications, among them 11 core theories that see widespread adoption in privacy research. A divide exists between the core theories, primarily focused on individual privacy, and the larger body of privacy research concerned with organisational and research contexts. We propose the Pillars of Privacy framework to bridge this gap, classifying the core theories on four levels of analysis along three pillars of privacy research: Privacy Concern, Privacy Calculus and Behavioural Outcomes.},
	author = {van Dijk, Friso Willem},
	file = {Pillars_of_Privacy__Identifying_Core_Theory_through_a_Network_Analysis_of_Privacy_Literature.pdf:/Users/joost/Zotero/storage/YJR8GGD2/Pillars_of_Privacy__Identifying_Core_Theory_through_a_Network_Analysis_of_Privacy_Literature.pdf:application/pdf},
}

@book{bird_natural_2009,
	title = {Natural {Language} {Processing} with {Python}: {Analyzing} {Text} with the {Natural} {Language} {Toolkit}},
	isbn = {978-0-596-55571-9},
	shorttitle = {Natural {Language} {Processing} with {Python}},
	abstract = {This book offers a highly accessible introduction to natural language processing, the field that supports a variety of language technologies, from predictive text and email filtering to automatic summarization and translation. With it, you'll learn how to write Python programs that work with large collections of unstructured text. You'll access richly annotated datasets using a comprehensive range of linguistic data structures, and you'll understand the main algorithms for analyzing the content and structure of written communication.Packed with examples and exercises, Natural Language Processing with Python will help you:Extract information from unstructured text, either to guess the topic or identify "named entities"Analyze linguistic structure in text, including parsing and semantic analysisAccess popular linguistic databases, including WordNet and treebanksIntegrate techniques drawn from fields as diverse as linguistics and artificial intelligenceThis book will help you gain practical skills in natural language processing using the Python programming language and the Natural Language Toolkit (NLTK) open source library. If you're interested in developing web applications, analyzing multilingual news sources, or documenting endangered languages -- or if you're simply curious to have a programmer's perspective on how human language works -- you'll find Natural Language Processing with Python both fascinating and immensely useful.},
	language = {en},
	publisher = {"O'Reilly Media, Inc."},
	author = {Bird, Steven and Klein, Ewan and Loper, Edward},
	month = jun,
	year = {2009},
	keywords = {Computers / General, Computers / Programming Languages / General, Computers / Programming Languages / JavaScript, Computers / Programming Languages / Python, Computers / Software Development \& Engineering / General},
}

@article{lovins_development_nodate,
	title = {Development of a stemming algorithm},
	language = {en},
	author = {Lovins, Julie Beth},
	pages = {10},
	file = {Lovins - Development of a stemming algorithm.pdf:/Users/joost/Zotero/storage/B32VGJJ4/Lovins - Development of a stemming algorithm.pdf:application/pdf},
}

@article{miller_wordnet_1995,
	title = {{WordNet}: a lexical database for {English}},
	volume = {38},
	issn = {0001-0782},
	shorttitle = {{WordNet}},
	doi = {10.1145/219717.219748},
	abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].},
	number = {11},
	journal = {Communications of the ACM},
	author = {Miller, George A.},
	month = nov,
	year = {1995},
	pages = {39--41},
}

@article{perera_designing_2020,
	title = {Designing privacy-aware internet of things applications},
	volume = {512},
	issn = {0020-0255},
	doi = {10.1016/j.ins.2019.09.061},
	abstract = {Internet of Things (IoT) applications typically collect and analyse personal data that can be used to derive sensitive information about individuals. However, thus far, privacy concerns have not been explicitly considered in software engineering processes when designing IoT applications. With the advent of behaviour driven security mechanisms, failing to address privacy concerns in the design of IoT applications can also have security implications. In this paper, we explore how a Privacy-by-Design (PbD) framework, formulated as a set of guidelines, can help software engineers integrate data privacy considerations into the design of IoT applications. We studied the utility of this PbD framework by studying how software engineers use it to design IoT applications. We also explore the challenges in using the set of guidelines to influence the IoT applications design process. In addition to highlighting the benefits of having a PbD framework to make privacy features explicit during the design of IoT applications, our studies also surfaced a number of challenges associated with the approach. A key finding of our research is that the PbD framework significantly increases both novice and expert software engineers’ ability to design privacy into IoT applications.},
	language = {en},
	journal = {Information Sciences},
	author = {Perera, Charith and Barhamgi, Mahmoud and Bandara, Arosha K. and Ajmal, Muhammad and Price, Blaine and Nuseibeh, Bashar},
	month = feb,
	year = {2020},
	keywords = {Internet of things, Privacy by Design, Software engineering},
	pages = {238--257},
	file = {ScienceDirect Snapshot:/Users/joost/Zotero/storage/R3R2XRXY/S0020025519309120.html:text/html;Accepted Version:/Users/joost/Zotero/storage/9MTBYU7A/Perera et al. - 2020 - Designing privacy-aware internet of things applica.pdf:application/pdf},
}

@article{syed_exploring_2018,
	title = {Exploring {Symmetrical} and {Asymmetrical} {Dirichlet} {Priors} for {Latent} {Dirichlet} {Allocation}},
	volume = {12},
	issn = {1793-351X},
	doi = {10.1142/S1793351X18400184},
	abstract = {Latent Dirichlet Allocation (LDA) has gained much attention from researchers and is increasingly being applied to uncover underlying semantic structures from a variety of corpora. However, nearly all researchers use symmetrical Dirichlet priors, often unaware of the underlying practical implications that they bear. This research is the first to explore symmetrical and asymmetrical Dirichlet priors on topic coherence and human topic ranking when uncovering latent semantic structures from scientific research articles. More specifically, we examine the practical effects of several classes of Dirichlet priors on 2000 LDA models created from abstract and full-text research articles. Our results show that symmetrical or asymmetrical priors on the document–topic distribution or the topic–word distribution for full-text data have little effect on topic coherence scores and human topic ranking. In contrast, asymmetrical priors on the document–topic distribution for abstract data show a significant increase in topic coherence scores and improved human topic ranking compared to a symmetrical prior. Symmetrical or asymmetrical priors on the topic–word distribution show no real benefits for both abstract and full-text data.},
	number = {03},
	journal = {International Journal of Semantic Computing},
	author = {Syed, Shaheen and Spruit, Marco},
	month = sep,
	year = {2018},
	pages = {399--423},
	file = {Snapshot:/Users/joost/Zotero/storage/ETRZLHTP/S1793351X18400184.html:text/html;Submitted Version:/Users/joost/Zotero/storage/MQSDF3J6/Syed and Spruit - 2018 - Exploring Symmetrical and Asymmetrical Dirichlet P.pdf:application/pdf},
}

@inproceedings{syed_full-text_2017,
	title = {Full-{Text} or {Abstract}? {Examining} {Topic} {Coherence} {Scores} {Using} {Latent} {Dirichlet} {Allocation}},
	shorttitle = {Full-{Text} or {Abstract}?},
	doi = {10.1109/DSAA.2017.61},
	abstract = {This paper assesses topic coherence and human topic ranking of uncovered latent topics from scientific publications when utilizing the topic model latent Dirichlet allocation (LDA) on abstract and full-text data. The coherence of a topic, used as a proxy for topic quality, is based on the distributional hypothesis that states that words with similar meaning tend to co-occur within a similar context. Although LDA has gained much attention from machine-learning researchers, most notably with its adaptations and extensions, little is known about the effects of different types of textual data on generated topics. Our research is the first to explore these practical effects and shows that document frequency, document word length, and vocabulary size have mixed practical effects on topic coherence and human topic ranking of LDA topics. We furthermore show that large document collections are less affected by incorrect or noise terms being part of the topic-word distributions, causing topics to be more coherent and ranked higher. Differences between abstract and full-text data are more apparent within small document collections, with differences as large as 90\% high-quality topics for full-text data, compared to 50\% high-quality topics for abstract data.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Data} {Science} and {Advanced} {Analytics} ({DSAA})},
	author = {Syed, S. and Spruit, M.},
	month = oct,
	year = {2017},
	keywords = {latent Dirichlet allocation, Abstract, abstract data, Adaptation models, Coherence, Data models, document collections, Full-Text, full-text data, high-quality topics, human topic ranking, Latent Dirichlet Allocation, LDA topics, learning (artificial intelligence), machine-learning researchers, mixed practical effects, Probabilistic logic, Resource management, Semantics, text analysis, topic coherence, Topic Coherence, topic coherence scores, topic model, topic quality, topic-word distributions, uncovered latent topics, Vocabulary},
	pages = {165--174},
	file = {IEEE Xplore Abstract Record:/Users/joost/Zotero/storage/E4QXKP92/8259775.html:text/html},
}

@inproceedings{wallach_rethinking_nodate,
  title={Rethinking LDA: Why priors matter},
  author={Wallach, Hanna M and Mimno, David M and McCallum, Andrew},
  booktitle={Advances in neural information processing systems},
  pages={1973--1981},
  year={2009}
}

@article{huang_maximum_2005,
	title = {Maximum {Likelihood} {Estimation} of {Dirichlet} {Distribution} {Parameters}},
	abstract = {Dirichlet distributions are commonly used as priors over propor- tional data. In this paper, I will introduce this distribution, discuss why it is useful, and compare implementations of 4 different methods for estimating its parameters from observed data.},
	author = {Huang, Jonathan},
	month = jan,
	year = {2005},
}

@inproceedings{hoffman_online_2010,
	title = {Online learning for latent {Dirichlet} allocation},
	abstract = {We develop an online variational Bayes (VB) algorithm for Latent Dirichlet Al-location (LDA). Online LDA is based on online stochastic optimization with a natural gradient step, which we show converges to a local optimum of the VB objective function. It can handily analyze massive document collections, includ-ing those arriving in a stream. We study the performance of online LDA in several ways, including by fitting a 100-topic topic model to 3.3M articles from Wikipedia in a single pass. We demonstrate that online LDA finds topic models as good or better than those found with batch VB, and in a fraction of the time. 1},
	booktitle = {In {Advances} in {Neural} {Information} {Processing} {Systems} 23 ({NIPS} ’10},
	author = {Hoffman, Matthew D. and Blei, David M. and Bach, Francis},
	year = {2010},
}

@article{boyd-graber_care_nodate,
  title={Care and feeding of topic models: Problems, diagnostics, and improvements},
  author={Boyd-Graber, Jordan and Mimno, David and Newman, David},
  journal={Handbook of mixed membership models and their applications},
  volume={225255},
  year={2014},
  publisher={CRC Press Boca Raton, FL}
}

@inproceedings{scott_recursive_2013,
	title = {A recursive estimate for the predictive likelihood in a topic model},
	abstract = {We consider the problem of evaluating the predictive log likelihood of a previously un- seen document under a topic model. This task arises when cross-validating for a model hyperparameter, when te...},
	language = {en},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Scott, James and Baldridge, Jason},
	month = apr,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {527--535},
	file = {Full Text PDF:/Users/joost/Zotero/storage/8V3U25ER/Scott and Baldridge - 2013 - A recursive estimate for the predictive likelihood.pdf:application/pdf;Snapshot:/Users/joost/Zotero/storage/JND4IU3M/scott13a.html:text/html},
}

@inproceedings{roder_exploring_2015,
	address = {New York, NY, USA},
	series = {{WSDM} '15},
	title = {Exploring the {Space} of {Topic} {Coherence} {Measures}},
	isbn = {978-1-4503-3317-7},
	doi = {10.1145/2684822.2685324},
	abstract = {Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from different sciences. The special case of measuring coherence of topics has been recently studied to remedy the problem that topic models give no guaranty on the interpretablity of their output. Several benchmark datasets were produced that record human judgements of the interpretability of topics. We are the first to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining elementary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. nFinally, we outline how our results can be transferred to further applications in the context of text mining, information retrieval and the world wide web.},
	booktitle = {Proceedings of the {Eighth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Röder, Michael and Both, Andreas and Hinneburg, Alexander},
	month = feb,
	year = {2015},
	keywords = {topic coherence, topic model, topic evaluation},
	pages = {399--408},
}

@article{warren_right_1890,
	title = {Right to {Privacy}},
	volume = {4},
	journal = {Harvard Law Review},
	author = {Warren, Samuel D. and Brandeis, Louis D.},
	year = {1890},
	pages = {193},
	file = {Right to Privacy 4 Harvard Law Review 1890-1891:/Users/joost/Zotero/storage/5J6DJRHD/LandingPage.html:text/html},
}

@article{kokolakis_privacy_2017,
	title = {Privacy attitudes and privacy behaviour: {A} review of current research on the privacy paradox phenomenon},
	volume = {64},
	issn = {0167-4048},
	shorttitle = {Privacy attitudes and privacy behaviour},
	doi = {10.1016/j.cose.2015.07.002},
	abstract = {Do people really care about their privacy? Surveys show that privacy is a primary concern for citizens in the digital age. On the other hand, individuals reveal personal information for relatively small rewards, often just for drawing the attention of peers in an online social network. This inconsistency of privacy attitudes and privacy behaviour is often referred to as the “privacy paradox”. In this paper, we present the results of a review of research literature on the privacy paradox. We analyse studies that provide evidence of a paradoxical dichotomy between attitudes and behaviour and studies that challenge the existence of such a phenomenon. The diverse research results are explained by the diversity in research methods, the different contexts and the different conceptualisations of the privacy paradox. We also present several interpretations of the privacy paradox, stemming from social theory, psychology, behavioural economics and, in one case, from quantum theory. We conclude that current research has improved our understanding of the privacy paradox phenomenon. It is, however, a complex phenomenon that requires extensive further research. Thus, we call for synthetic studies to be based on comprehensive theoretical models that take into account the diversity of personal information and the diversity of privacy concerns. We suggest that future studies should use evidence of actual behaviour rather than self-reported behaviour.},
	language = {en},
	journal = {Computers \& Security},
	author = {Kokolakis, Spyros},
	month = jan,
	year = {2017},
	keywords = {Information privacy, Personal information, Privacy, Privacy behaviour, Privacy paradox},
	pages = {122--134},
	file = {ScienceDirect Snapshot:/Users/joost/Zotero/storage/ZQUIIVM6/S0167404815001017.html:text/html},
}

@inproceedings{sievert2014ldavis,
  title={LDAvis: A method for visualizing and interpreting topics},
  author={Sievert, Carson and Shirley, Kenneth},
  booktitle={Proceedings of the workshop on interactive language learning, visualization, and interfaces},
  pages={63--70},
  year={2014}
}

@phdthesis{noyons_bibliometric_1999,
	title = {Bibliometric mapping as a science policy and research management tool},
	abstract = {Bibliometric maps of science are landscapes of scientific research fields created by quantitative analysis of bibliographic data. In such maps the 'cities' are, for instance, research topics. Topics with a strong cognitive relation are in each other's vicinity and topics with a weak relation are distant from each other. These maps have several domains of application. As a policy supportive tool they can be applied to overview the structure of a research field and to monitor its evolution. This book contributes to the development of this application of bibliometric maps.},
	language = {en},
	school = {DSWO Press},
	author = {Noyons, E. C. M.},
	month = dec,
	year = {1999},
	note = {ISBN: 9789090132501},
	file = {Full Text PDF:/Users/joost/Zotero/storage/3X3GVFZT/Noyons - 1999 - Bibliometric mapping as a science policy and resea.pdf:application/pdf;Snapshot:/Users/joost/Zotero/storage/GMDALC82/38308.html:text/html},
}

@article{mccallum_malletmachine_2002,
	title = {{MALLET}:{A} {Machine} {Learning} for {Language} {Toolkit}},
	shorttitle = {{MALLET}},
	journal = {http://mallet.cs.umass.edu},
	author = {MCCALLUM, A.K.},
	year = {2002},
	file = {MALLET\:A Machine Learning for Language Toolkit Snapshot:/Users/joost/Zotero/storage/AZ598TAC/20001704926.html:text/html},
}

@misc{gadellaa_networked_2019,
	type = {Bachelor thesis},
	title = {A {Networked} {Evolutionary} {Trust} {Game} for the {Sharing} {Economy}},
	copyright = {Open Access (free)},
	abstract = {In the sharing economy, trust is of higher importance than in regular B2C interactions because there is no transfer of ownership and transactions sometimes take place in private space. Chica et al. (2017) developed an evolutionary trust game to unveil occurring dynamics and explain how trust could evolve in the sharing economy. This thesis adds the variable of network structure for the network of possible interactions to their model. Using agent-based modelling, the model is ran on networks with varying levels of community structure, systematically varying average degree, the community connectedness and game payoffs. We find negative correlations for degree and it is suggested that having more isolated communities has a positive effect on trust, but this is highly dependent on the reward for cooperating. Chica et al. (2017) showed strong interdependence between players of different strategies and found a strong influence of payoffs on the dynamics. The current research shows that a strong interaction with network structure should also be considered. The presented findings progress the field of evolutionary game theory, by learning from a specific application. Furthermore, findings also suggest that sharing economy platforms could enhance trust by emphasising or creating communities, depending on the risk and how clear benefits are to users.},
	language = {en},
	author = {Gadellaa, J. F.},
	year = {2019},
	note = {Accepted: 2020-02-20T19:07:14Z},
	file = {Full Text PDF:/Users/joost/Zotero/storage/6MAFPL8B/Gadellaa - 2019 - A Networked Evolutionary Trust Game for the Sharin.pdf:application/pdf;Snapshot:/Users/joost/Zotero/storage/SYI5QSE5/393877.html:text/html},
}

@article{baradad_corpus_nodate,
	title = {Corpus {Specific} {Stop} {Words} to {Improve} the {Textual} {Analysis} in {Scientometrics}},
	booktitle={ISSI},
    year={2015},
	abstract = {With the availability of vast collection of research articles on internet, textual analysis is an increasingly important technique in scientometric analysis. While the context in which it is used and the specific algorithms implemented may vary, typically any textual analysis exercise involves intensive pre-processing of input text which includes removing topically uninteresting terms (stop words). In this paper we argue that corpus specific stop words, which take into account the specificities of a collection of texts, improve textual analysis in scientometrics. We describe two relatively simple techniques to generate corpus-specific stop words; stop words lists following a Poisson distribution and keyword adjacency stop words lists. In a case study to extract keywords from scientific abstracts of research project funded by the European Research Council in the domain of Life sciences, we show that a combination of those techniques gives better recall values than standard stop words or any of the two techniques alone. The method we propose can be implemented to obtain stop words lists in an automatic way by using author provided keywords for a set of abstracts. The stop words lists generated can be updated easily by adding new texts to the training corpus.},
	language = {en},
	author = {Baradad, Vicenç Parisi and Mugabushaka, Alexis-Michel},
	pages = {7},
	file = {Baradad and Mugabushaka - Corpus Specific Stop Words to Improve the Textual .pdf:/Users/joost/Zotero/storage/TW87D3YB/Baradad and Mugabushaka - Corpus Specific Stop Words to Improve the Textual .pdf:application/pdf},
}

@inproceedings{wallach_topic_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {Topic modeling: beyond bag-of-words},
	isbn = {978-1-59593-383-6},
	shorttitle = {Topic modeling},
	doi = {10.1145/1143844.1143967},
	abstract = {Some models of textual corpora employ text generation methods involving n-gram statistics, while others use latent topic variables inferred using the "bag-of-words" assumption, in which word order is ignored. Previously, these methods have not been combined. In this work, I explore a hierarchical generative probabilistic model that incorporates both n-gram statistics and latent topic variables by extending a unigram topic model to include properties of a hierarchical Dirichlet bigram language model. The model hyperparameters are inferred using a Gibbs EM algorithm. On two data sets, each of 150 documents, the new model exhibits better predictive accuracy than either a hierarchical Dirichlet bigram language model or a unigram topic model. Additionally, the inferred topics are less dominated by function words than are topics discovered using unigram statistics, potentially making them more meaningful.},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Wallach, Hanna M.},
	month = jun,
	year = {2006},
	pages = {977--984},
}

@inproceedings{blei_dynamic_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {Dynamic topic models},
	isbn = {978-1-59593-383-6},
	doi = {10.1145/1143844.1143859},
	abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
	publisher = {Association for Computing Machinery},
	author = {Blei, David M. and Lafferty, John D.},
	month = jun,
	year = {2006},
	pages = {113--120},
}

@article{noauthor_quantitative_nodate,
  title={Quantitative analysis of large amounts of journalistic texts using topic modelling},
  author={Jacobi, Carina and Van Atteveldt, Wouter and Welbers, Kasper},
  journal={Digital Journalism},
  volume={4},
  number={1},
  pages={89--106},
  year={2016},
  publisher={Taylor \& Francis}
}

@article{jiang_topic_2016,
	title = {A topic modeling based bibliometric exploration of hydropower research},
	volume = {57},
	issn = {1364-0321},
	doi = {10.1016/j.rser.2015.12.194},
	abstract = {Scientific research articles can provide rich insights into practitioners׳ viewpoints around contentious policy making. Although much attention has been paid to hydropower development in the literature, few of them gathered systematic data and performed a large-scale review of scientific articles. In this study, we employed a topic modeling based bibliometric analysis to quantitatively evaluate global scientific literature of hydropower, with a time frame from 1994 to 2013. We analyzed 1726 scholarly articles highly related to hydropower, to discover the research development, current trends and intellectual structure of hydropower literature. Common bibliometric indicators show that hydropower research publications sustain a rapid growth rate, English is the dominant language, and the hotspots of hydropower research can be concluded as “fish”, “species”, “climate”, “emission”, “lake”, “sediment”, “Turkey”, etc. We established a 29-topic model to describe the intellectual structure of the 1726 articles, and employed cluster analysis and trend analysis to process the derived topics. We find that post construction issues of hydropower are more attractive for scholars than construction technology itself, and an interdisciplinary trend of hydropower research is emerging. The methodology reported in this study is expected to gain traction as a methodological strategy for energy research reviews and subsequently to promote energy policy making.},
	language = {en},
	journal = {Renewable and Sustainable Energy Reviews},
	author = {Jiang, Hanchen and Qiang, Maoshan and Lin, Peng},
	month = may,
	year = {2016},
	keywords = {Bibliometrics, Hydropower, Renewable energy, Research trends, Topic modeling},
	pages = {226--237},
	file = {ScienceDirect Snapshot:/Users/joost/Zotero/storage/PXWZTGT7/S1364032115015774.html:text/html},
}

@inproceedings{mimno2006bibliometric,
  title={Bibliometric impact measures leveraging topic analysis},
  author={Mimno, David and McCallum, Andrew and Mann, Gideon S},
  booktitle={Proceedings of the 6th ACM/IEEE-CS Joint Conference on Digital libraries (JCDL'06)},
  pages={65--74},
  year={2006},
  organization={IEEE}
}

@article{haselmayer2014measuring,
  title={Measuring the tonality of negative campaigning: Combining a dictionary approach with crowd-coding},
  author={Haselmayer, Martin and Jenny, Marcelo},
  journal={political context Matters: Content analysis in the social sciences, Mannheim, Germany},
  year={2014}
}

@article{grimmer2013text,
  title={Text as data: The promise and pitfalls of automatic content analysis methods for political texts},
  author={Grimmer, Justin and Stewart, Brandon M},
  journal={Political analysis},
  volume={21},
  number={3},
  pages={267--297},
  year={2013},
  publisher={Cambridge University Press}
}

@book{bird2009natural,
  title={Natural language processing with Python: analyzing text with the natural language toolkit},
  author={Bird, Steven and Klein, Ewan and Loper, Edward},
  year={2009},
  publisher={" O'Reilly Media, Inc."}
}

@inproceedings{scott1999feature,
  title={Feature engineering for text classification},
  author={Scott, Sam and Matwin, Stan},
  booktitle={ICML},
  volume={99},
  pages={379--388},
  year={1999},
  organization={Citeseer}
}

@article{denny2018text,
  title={Text preprocessing for unsupervised learning: Why it matters, when it misleads, and what to do about it},
  author={Denny, Matthew J and Spirling, Arthur},
  journal={Political Analysis},
  volume={26},
  number={2},
  pages={168--189},
  year={2018},
  publisher={Cambridge University Press}
}

@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={the Journal of machine Learning research},
  volume={3},
  pages={993--1022},
  year={2003},
  publisher={JMLR. org}
}
@article{elgesem2015structure,
  title={Structure and content of the discourse on climate change in the blogosphere: The big picture},
  author={Elgesem, Dag and Steskal, Lubos and Diakopoulos, Nicholas},
  journal={Environmental Communication},
  volume={9},
  number={2},
  pages={169--188},
  year={2015},
  publisher={Taylor \& Francis}
}
@article{baas2020scopus,
  title={Scopus as a curated, high-quality bibliometric data source for academic research in quantitative science studies},
  author={Baas, Jeroen and Schotten, Michiel and Plume, Andrew and C{\^o}t{\'e}, Gr{\'e}goire and Karimi, Reza},
  journal={Quantitative Science Studies},
  volume={1},
  number={1},
  pages={377--386},
  year={2020},
  publisher={MIT Press}
}
@article{rose2019pybliometrics,
  title={pybliometrics: Scriptable bibliometrics using a Python interface to Scopus},
  author={Rose, Michael E and Kitchin, John R},
  journal={SoftwareX},
  volume={10},
  pages={100263},
  year={2019},
  publisher={Elsevier}
}
@article{westerlund_topic_2018,
	title = {A {Topic} {Modelling} {Analysis} of {Living} {Labs} {Research}},
	volume = {8},
	issn = {1927-0321},
	doi = {http://doi.org/10.22215/timreview/1170},
	number = {7},
	journal = {Technology Innovation Management Review},
	author = {Westerlund, Mika and Leminen, Seppo and Rajahonka, Mervi},
	year = {2018},
	pages = {40--51},
	file = {A Topic Modelling Analysis of Living Labs Research | TIM Review:/Users/joost/Zotero/storage/ZM3KZZ4J/1170.html:text/html;Full Text:/Users/joost/Zotero/storage/LLGAJ255/Westerlund et al. - 2018 - A Topic Modelling Analysis of Living Labs Research.pdf:application/pdf},
}
@inproceedings{noauthor_exploring_nodate,
  title={Exploring the space of topic coherence measures},
  author={R{\"o}der, Michael and Both, Andreas and Hinneburg, Alexander},
  booktitle={Proceedings of the eighth ACM international conference on Web search and data mining},
  pages={399--408},
  url = {https://dl.acm.org/doi/abs/10.1145/2684822.2685324},
  year={2015}
}

@inproceedings{mimno_bibliometric_2006,
	title = {Bibliometric impact measures leveraging topic analysis},
	doi = {10.1145/1141753.1141765},
	abstract = {Measurements of the impact and history of research literature provide a useful complement to scientific digital library collections. Bibliometric indicators have been extensively studied, mostly in the context of journals. However, journal-based metrics poorly capture topical distinctions in fast-moving fields, and are increasingly problematic with the rise of open-access publishing. Recent developments in latent topic models have produced promising results for automatic sub-field discovery. The fine-grained, faceted topics produced by such models provide a clearer view of the topical divisions of a body of research literature and the interactions between those divisions. We demonstrate the usefulness of topic models in measuring impact by applying a new phrase-based topic discovery model to a collection of 300,000 computer science publications, collected by the Rexa automatic citation indexing system},
	booktitle = {Proceedings of the 6th {ACM}/{IEEE}-{CS} {Joint} {Conference} on {Digital} {Libraries} ({JCDL} '06)},
	author = {Mimno, D. and McCallum, A. and Mann, G. S.},
	month = jun,
	year = {2006},
	keywords = {Bibliometrics, Computer science, Context awareness, data curation, harvesting, History, Indexing, Information retrieval, meta-search, Permission, Publishing, search and retrieval protocols, Software libraries, subject portals, Text processing, web services},
	pages = {65--74},
	file = {IEEE Xplore Abstract Record:/Users/joost/Zotero/storage/LZ5Y87CD/4119099.html:text/html;Full Text:/Users/joost/Zotero/storage/I9RWCBII/Mimno et al. - 2006 - Bibliometric impact measures leveraging topic anal.pdf:application/pdf},
}

@article{rehurek_gensimpython_2011,
	title = {Gensim–python framework for vector space modelling},
	volume = {3},
	number = {2},
	journal = {NLP Centre, Faculty of Informatics, Masaryk University, Brno, Czech Republic},
	author = {Rehurek, Radim and Sojka, Petr},
	year = {2011},
}